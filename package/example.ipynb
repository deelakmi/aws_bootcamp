{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0W3We6iLO--"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/yandex-research/rtdl-revisiting-models/blob/main/package/example.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "---\n",
        "\n",
        "**See also** [RTDL](https://github.com/yandex-research/rtdl)\n",
        "-- **other projects on tabular deep learning**.\n",
        "\n",
        "---\n",
        "\n",
        "- This notebook provides a usage example of the\n",
        "  [rtdl_revisiting_models](https://github.com/yandex-research/rtdl-revisiting-models)\n",
        "  package.\n",
        "- Hyperparameters are not tuned and may be suboptimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H83_0F3NLO_A",
        "outputId": "748cc0f9-51b4-4f0f-aaf9-3678a10a4e63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: delu==0.0.23 in /usr/local/lib/python3.10/dist-packages (0.0.23)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.10/dist-packages (from delu==0.0.23) (1.26.4)\n",
            "Requirement already satisfied: torch<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from delu==0.0.23) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->delu==0.0.23) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->delu==0.0.23) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->delu==0.0.23) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->delu==0.0.23) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->delu==0.0.23) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->delu==0.0.23) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=1.8->delu==0.0.23) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.8->delu==0.0.23) (3.0.2)\n",
            "Requirement already satisfied: rtdl_revisiting_models in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from rtdl_revisiting_models) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=1.8->rtdl_revisiting_models) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.8->rtdl_revisiting_models) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install delu==0.0.23\n",
        "%pip install rtdl_revisiting_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wqiAQtVKLO_B"
      },
      "outputs": [],
      "source": [
        "# ruff: noqa: E402\n",
        "import math\n",
        "import warnings\n",
        "from typing import Dict, Literal\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import delu  # Deep Learning Utilities: https://github.com/Yura52/delu\n",
        "import numpy as np\n",
        "import scipy.special\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "from torch import Tensor\n",
        "from tqdm.std import tqdm\n",
        "\n",
        "warnings.resetwarnings()\n",
        "\n",
        "from rtdl_revisiting_models import MLP, ResNet, FTTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wyz0SowqLO_B",
        "outputId": "679b5c1d-9058-41c7-c628-748cc7de9353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b9fc4efae656>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Set random seeds in all libraries.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/delu/random.py\u001b[0m in \u001b[0;36mseed\u001b[0;34m(base_seed, one_cuda_seed)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Set random seeds in all libraries.\n",
        "delu.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD2tJ6GtLO_C"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset_id = 1590   #45068\n",
        "dataset = openml.datasets.get_dataset(dataset_id)\n",
        "X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "full_data = X.copy()\n",
        "full_data[dataset.default_target_attribute] = y\n",
        "\n",
        "csv_file = 'heloc.csv'\n",
        "full_data.to_csv(csv_file, index=False)\n",
        "\n",
        "print(f\"Dataset saved as {csv_file}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c9o-IQCSQtnF",
        "outputId": "ceb9d327-709e-42e0-d77d-7c8b062fd0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'openml' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f9e5d0e3994f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1590\u001b[0m   \u001b[0;31m#45068\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_target_attribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfull_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfull_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_target_attribute\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'openml' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Fetch dataset from OpenML\n",
        "TaskType = Literal[\"regression\", \"binclass\", \"multiclass\"]\n",
        "\n",
        "task_type: TaskType = \"binclass\"\n",
        "n_classes = None\n",
        "#dataset = sklearn.datasets.fetch_california_housing()\n",
        "dataset = fetch_openml(data_id=1590, as_frame=False)\n",
        "X_cont: np.ndarray = dataset[\"data\"]\n",
        "Y: np.ndarray = dataset[\"target\"]\n",
        "\n",
        "# Check the dtype of the target\n",
        "print(f\"Original dtype of Y: {Y.dtype}\")\n",
        "\n",
        "# Map class '1' to '0' and class '2' to '1' while preserving object dtype\n",
        "mapping = {'<=50K': '0', '2': '1'}\n",
        "Y = np.array([mapping[str(y)] if str(y) in mapping else y for y in Y], dtype=object)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nLHfhIgFMXWa",
        "outputId": "c96555a2-fe6f-4af1-992d-8a60b2eb78e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dtype of Y: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a3qLhNMuLO_C",
        "outputId": "40e89e8d-0a20-434c-831d-8b382c1a0f71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dtype of Y: object\n",
            "X_cat dtype: object\n",
            "Example categorical data: [['Private' '11th' 'Never-married' 'Machine-op-inspct' 'Own-child'\n",
            "  'Black' 'Male' 'United-States']\n",
            " ['Private' 'HS-grad' 'Married-civ-spouse' 'Farming-fishing' 'Husband'\n",
            "  'White' 'Male' 'United-States']\n",
            " ['Local-gov' 'Assoc-acdm' 'Married-civ-spouse' 'Protective-serv'\n",
            "  'Husband' 'White' 'Male' 'United-States']\n",
            " ['Private' 'Some-college' 'Married-civ-spouse' 'Machine-op-inspct'\n",
            "  'Husband' 'Black' 'Male' 'United-States']\n",
            " [nan 'Some-college' 'Never-married' nan 'Own-child' 'White' 'Female'\n",
            "  'United-States']]\n",
            "Cardinalities of categorical features: [9, 16, 7, 15, 6, 5, 2, 42]\n",
            "Cardinalities of categorical features: [9, 16, 7, 15, 6, 5, 2, 42]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-298e43170515>:55: RuntimeWarning: invalid value encountered in cast\n",
            "  X_cat = X_cat.astype(np.int64)  # <- Change here\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Fetch dataset from OpenML\n",
        "TaskType = Literal[\"regression\", \"binclass\", \"multiclass\"]\n",
        "\n",
        "task_type: TaskType = \"binclass\"\n",
        "n_classes = 2\n",
        "#dataset = sklearn.datasets.fetch_california_housing()\n",
        "dataset = fetch_openml(data_id=1590, as_frame=False)\n",
        "X: np.ndarray = dataset[\"data\"]\n",
        "Y: np.ndarray = dataset[\"target\"]\n",
        "\n",
        "# Check the dtype of the target\n",
        "print(f\"Original dtype of Y: {Y.dtype}\")\n",
        "\n",
        "# Map class '1' to '0' and class '2' to '1' while preserving object dtype\n",
        "mapping = {'<=50K': '0', '>50K': '1'}\n",
        "Y = np.array([mapping[str(y)] if str(y) in mapping else y for y in Y], dtype=object)\n",
        "# NOTE: uncomment to solve a classification task.\n",
        "# n_classes = 2\n",
        "#assert n_classes >= 2\n",
        "# task_type: TaskType = 'binclass' if n_classes == 2 else 'multiclass'\n",
        "# X_cont, Y = sklearn.datasets.make_classification(\n",
        "#     n_samples=20000,\n",
        "#     n_features=8,\n",
        "#     n_classes=n_classes,\n",
        "#     n_informative=3,\n",
        "#     n_redundant=2,\n",
        "# )\n",
        "\n",
        "numerical_indices = [0, 2, 4, 10, 11,12]  # Replace with actual indices of numerical features\n",
        "categorical_indices = [1, 3, 5, 6,7,8,9,13]\n",
        "X_cont: np.ndarray = X[:, numerical_indices]\n",
        "X_cat: np.ndarray = X[:, categorical_indices] if categorical_indices else None\n",
        "X_cont: np.ndarray = X_cont.astype(np.float32)\n",
        "n_cont_features = X_cont.shape[1]\n",
        "\n",
        "if X_cat is not None:\n",
        "    print(f\"X_cat dtype: {X_cat.dtype}\")\n",
        "    print(\"Example categorical data:\", X_cat[:5])\n",
        "else:\n",
        "    print(\"No categorical features.\")\n",
        "\n",
        "if X_cat is not None:\n",
        "    # Handle NaN values (replace NaN with a placeholder, e.g., \"missing\")\n",
        "    X_cat = np.where(X_cat == np.array(None), \"missing\", X_cat)\n",
        "    X_cat = np.where(X_cat == np.nan, \"missing\", X_cat)\n",
        "\n",
        "    # Use OrdinalEncoder to convert categories to integers\n",
        "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "    X_cat = encoder.fit_transform(X_cat)\n",
        "\n",
        "    # Convert to integers for compatibility with embedding layers\n",
        "    X_cat = X_cat.astype(np.int64)  # <- Change here\n",
        "\n",
        "cat_cardinalities = [len(np.unique(X_cat[:, col])) for col in range(X_cat.shape[1])]\n",
        "print(f\"Cardinalities of categorical features: {cat_cardinalities}\")\n",
        "\n",
        "\n",
        "cat_cardinalities = [len(np.unique(X_cat[:, col])) for col in range(X_cat.shape[1])]\n",
        "print(f\"Cardinalities of categorical features: {cat_cardinalities}\")\n",
        "# >>> Categorical features.\n",
        "# NOTE: the above datasets do not have categorical features, but,\n",
        "# for the demonstration purposes, it is possible to generate them.\n",
        "#cat_cardinalities = [\n",
        "    # NOTE: uncomment the two lines below to add two categorical features.\n",
        "    # 4,  # Allowed values: [0, 1, 2, 3].\n",
        "    # 7,  # Allowed values: [0, 1, 2, 3, 4, 5, 6].\n",
        "#]\n",
        "# X_cat = (\n",
        "#     np.column_stack(\n",
        "#         [np.random.randint(0, c, (len(X_cont),)) for c in cat_cardinalities]\n",
        "#     )\n",
        "#     if cat_cardinalities\n",
        "#     else None\n",
        "# )\n",
        "\n",
        "# >>> Labels.\n",
        "# Regression labels must be represented by float32.\n",
        "if task_type == \"regression\":\n",
        "    Y = Y.astype(np.float32)\n",
        "else:\n",
        "    assert n_classes is not None\n",
        "    Y = Y.astype(np.int64)\n",
        "    assert set(Y.tolist()) == set(\n",
        "        range(n_classes)\n",
        "    ), \"Classification labels must form the range [0, 1, ..., n_classes - 1]\"\n",
        "\n",
        "# >>> Split the dataset.\n",
        "all_idx = np.arange(len(Y))\n",
        "trainval_idx, test_idx = sklearn.model_selection.train_test_split(\n",
        "    all_idx, train_size=0.8\n",
        ")\n",
        "train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
        "    trainval_idx, train_size=0.8125\n",
        ")\n",
        "data_numpy = {\n",
        "    \"train\": {\"x_cont\": X_cont[train_idx], \"y\": Y[train_idx]},\n",
        "    \"val\": {\"x_cont\": X_cont[val_idx], \"y\": Y[val_idx]},\n",
        "    \"test\": {\"x_cont\": X_cont[test_idx], \"y\": Y[test_idx]},\n",
        "}\n",
        "if X_cat is not None:\n",
        "    data_numpy[\"train\"][\"x_cat\"] = X_cat[train_idx]\n",
        "    data_numpy[\"val\"][\"x_cat\"] = X_cat[val_idx]\n",
        "    data_numpy[\"test\"][\"x_cat\"] = X_cat[test_idx]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of features (X_cont) and target (Y)\n",
        "print(dataset[\"data\"].shape)  # Shape of the features (e.g., (20640, 8))\n",
        "print(dataset[\"target\"].shape)  # Shape of the target (e.g., (20640,))\n"
      ],
      "metadata": {
        "id": "uzKZOpPVLf1d",
        "outputId": "0f652711-2ea2-456e-bcc1-b8a09078e061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48842, 14)\n",
            "(48842,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of features (X_cont) and target (Y)\n",
        "print(dataset[\"data\"].shape)  # Shape of the features (e.g., (20640, 8))\n",
        "print(dataset[\"target\"].shape)  # Shape of the target (e.g., (20640,))"
      ],
      "metadata": {
        "id": "MPbTCXEWPRT6",
        "outputId": "09e175cf-d236-4222-d811-2b2dc3367297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48842, 14)\n",
            "(48842,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWszqpViLO_C"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HwlPYWoJLO_C"
      },
      "outputs": [],
      "source": [
        "# >>> Feature preprocessing.\n",
        "# NOTE\n",
        "# The choice between preprocessing strategies depends on a task and a model.\n",
        "\n",
        "# (A) Simple preprocessing strategy.\n",
        "# preprocessing = sklearn.preprocessing.StandardScaler().fit(\n",
        "#     data_numpy['train']['x_cont']\n",
        "# )\n",
        "\n",
        "# (B) Fancy preprocessing strategy.\n",
        "# The noise is added to improve the output of QuantileTransformer in some cases.\n",
        "X_cont_train_numpy = data_numpy[\"train\"][\"x_cont\"]\n",
        "noise = (\n",
        "    np.random.default_rng(0)\n",
        "    .normal(0.0, 1e-5, X_cont_train_numpy.shape)\n",
        "    .astype(X_cont_train_numpy.dtype)\n",
        ")\n",
        "preprocessing = sklearn.preprocessing.QuantileTransformer(\n",
        "    n_quantiles=max(min(len(train_idx) // 30, 1000), 10),\n",
        "    output_distribution=\"normal\",\n",
        "    subsample=10**9,\n",
        ").fit(X_cont_train_numpy + noise)\n",
        "del X_cont_train_numpy\n",
        "\n",
        "for part in data_numpy:\n",
        "    data_numpy[part][\"x_cont\"] = preprocessing.transform(data_numpy[part][\"x_cont\"])\n",
        "\n",
        "# >>> Label preprocessing.\n",
        "if task_type == \"regression\":\n",
        "    Y_mean = data_numpy[\"train\"][\"y\"].mean().item()\n",
        "    Y_std = data_numpy[\"train\"][\"y\"].std().item()\n",
        "    for part in data_numpy:\n",
        "        data_numpy[part][\"y\"] = (data_numpy[part][\"y\"] - Y_mean) / Y_std\n",
        "\n",
        "# >>> Convert data to tensors.\n",
        "data = {\n",
        "    part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
        "    for part in data_numpy\n",
        "}\n",
        "\n",
        "if task_type != \"multiclass\":\n",
        "    # Required by F.binary_cross_entropy_with_logits\n",
        "    for part in data:\n",
        "        data[part][\"y\"] = data[part][\"y\"].float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8LGp4p2LO_C"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jGl4VH5VLO_D",
        "outputId": "d2278f8c-595d-4987-9788-4b3132a3840d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ee4f083cb374>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0md_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mFTTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m ).to(device)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_default_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "# The output size.\n",
        "d_out = n_classes if task_type == \"multiclass\" else 1\n",
        "\n",
        "# # NOTE: uncomment to train MLP\n",
        "# model = MLP(\n",
        "#     d_in=n_cont_features + sum(cat_cardinalities),\n",
        "#     d_out=d_out,\n",
        "#     n_blocks=2,\n",
        "#     d_block=384,\n",
        "#     dropout=0.1,\n",
        "# ).to(device)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "\n",
        "# # NOTE: uncomment to train ResNet\n",
        "# model = ResNet(\n",
        "#     d_in=n_cont_features + sum(cat_cardinalities),\n",
        "#     d_out=d_out,\n",
        "#     n_blocks=2,\n",
        "#     d_block=192,\n",
        "#     d_hidden=None,\n",
        "#     d_hidden_multiplier=2.0,\n",
        "#     dropout1=0.3,\n",
        "#     dropout2=0.0,\n",
        "# ).to(device)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "\n",
        "model = FTTransformer(\n",
        "    n_cont_features=n_cont_features,\n",
        "    cat_cardinalities=cat_cardinalities,\n",
        "    d_out=d_out,\n",
        "    **FTTransformer.get_default_kwargs(),\n",
        ").to(device)\n",
        "optimizer = model.make_default_optimizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjUcFiRCLO_D"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eCv3HOcoLO_D",
        "outputId": "e605129c-0e54-4594-c398-45c1e6208992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8bcb610111d3>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test score before training: {evaluate(\"test\"):.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-8bcb610111d3>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(part)\u001b[0m\n\u001b[1;32m     34\u001b[0m     y_pred = (\n\u001b[1;32m     35\u001b[0m         torch.cat(\n\u001b[0;32m---> 36\u001b[0;31m             [\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mapply_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-8bcb610111d3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m         torch.cat(\n\u001b[1;32m     36\u001b[0m             [\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mapply_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             ]\n",
            "\u001b[0;32m<ipython-input-12-8bcb610111d3>\u001b[0m in \u001b[0;36mapply_model\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         )\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x_cont\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_cat_ohe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFTTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "def apply_model(batch: Dict[str, Tensor]) -> Tensor:\n",
        "    if isinstance(model, (MLP, ResNet)):\n",
        "        x_cat_ohe = (\n",
        "            [\n",
        "                F.one_hot(column, cardinality)\n",
        "                for column, cardinality in zip(batch[\"x_cat\"].T, cat_cardinalities)\n",
        "            ]\n",
        "            if \"x_cat\" in batch\n",
        "            else []\n",
        "        )\n",
        "        return model(torch.column_stack([batch[\"x_cont\"]] + x_cat_ohe)).squeeze(-1)\n",
        "\n",
        "    elif isinstance(model, FTTransformer):\n",
        "        return model(batch[\"x_cont\"], batch.get(\"x_cat\")).squeeze(-1)\n",
        "\n",
        "    else:\n",
        "        raise RuntimeError(f\"Unknown model type: {type(model)}\")\n",
        "\n",
        "\n",
        "loss_fn = (\n",
        "    F.binary_cross_entropy_with_logits\n",
        "    if task_type == \"binclass\"\n",
        "    else F.cross_entropy\n",
        "    if task_type == \"multiclass\"\n",
        "    else F.mse_loss\n",
        ")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(part: str) -> float:\n",
        "    model.eval()\n",
        "\n",
        "    eval_batch_size = 8096\n",
        "    y_pred = (\n",
        "        torch.cat(\n",
        "            [\n",
        "                apply_model(batch)\n",
        "                for batch in delu.iter_batches(data[part], eval_batch_size)\n",
        "            ]\n",
        "        )\n",
        "        .cpu()\n",
        "        .numpy()\n",
        "    )\n",
        "    y_true = data[part][\"y\"].cpu().numpy()\n",
        "\n",
        "    if task_type == \"binclass\":\n",
        "        y_pred = np.round(scipy.special.expit(y_pred))\n",
        "        score = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "    elif task_type == \"multiclass\":\n",
        "        y_pred = y_pred.argmax(1)\n",
        "        score = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "    else:\n",
        "        assert task_type == \"regression\"\n",
        "        score = -(sklearn.metrics.mean_squared_error(y_true, y_pred) ** 0.5 * Y_std)\n",
        "    return score  # The higher -- the better.\n",
        "\n",
        "\n",
        "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 1_000_000_000\n",
        "patience = 30\n",
        "\n",
        "batch_size = 256\n",
        "epoch_size = math.ceil(len(train_idx) / batch_size)\n",
        "timer = delu.tools.Timer()\n",
        "early_stopping = delu.tools.EarlyStopping(patience, mode=\"max\")\n",
        "\n",
        "best = {\n",
        "    \"val\": -math.inf,\n",
        "    \"test\": None,  # Store test score only for the best validation score\n",
        "    \"epoch\": -1,\n",
        "}\n",
        "\n",
        "test_runs = 5  # Number of test evaluations to average\n",
        "\n",
        "print(f\"Device: {device.type.upper()}\")\n",
        "print(\"-\" * 88 + \"\\n\")\n",
        "timer.run()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for batch in tqdm(\n",
        "        delu.iter_batches(data[\"train\"], batch_size, shuffle=True),\n",
        "        desc=f\"Epoch {epoch}\",\n",
        "        total=epoch_size,\n",
        "    ):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(apply_model(batch), batch[\"y\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    val_score = evaluate(\"val\")\n",
        "\n",
        "    # Update early stopping\n",
        "    early_stopping.update(val_score)\n",
        "\n",
        "    # Check if early stopping indicates to stop\n",
        "    if early_stopping.should_stop():\n",
        "        print(\"\\nEarly stopping triggered. Evaluating test score for the best validation score...\\n\")\n",
        "\n",
        "        # Evaluate test score multiple times for the best validation score\n",
        "        test_scores = [evaluate(\"test\") for _ in range(test_runs)]\n",
        "        print(f\" the test score for epoch {epoch} is {test_scores}\")\n",
        "        average_test_score = sum(test_scores) / test_runs\n",
        "        best[\"test\"] = average_test_score\n",
        "\n",
        "        print(f\"Best validation score: {best['val']:.4f}\")\n",
        "        print(f\"Averaged Test score over {test_runs} runs: {best['test']:.4f}\")\n",
        "        break\n",
        "\n",
        "    # Check if current epoch has the best validation score\n",
        "    if val_score > best[\"val\"]:\n",
        "        print(\"🌸 New best epoch! 🌸\")\n",
        "        best = {\"val\": val_score, \"test\": None, \"epoch\": epoch}\n",
        "\n",
        "    print(f\"(val) {val_score:.4f} [time] {timer}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n\\nResult:\")\n",
        "print(f\"Best Epoch: {best['epoch']}\")\n",
        "print(f\"Validation Score: {best['val']:.4f}\")\n",
        "if best[\"test\"] is not None:\n",
        "    print(f\"Averaged Test Score: {best['test']:.4f}\")\n",
        "else:\n",
        "    print(\"Test score was not evaluated.\")\n"
      ],
      "metadata": {
        "id": "bvDhDh_hHaNY",
        "outputId": "07590d67-e8f4-4ab1-cc6c-98bdabb920d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: CUDA\n",
            "----------------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 26/26 [00:00<00:00, 309.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌸 New best epoch! 🌸\n",
            "(val) 0.6467 [time] 0:00:00.092546\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 26/26 [00:00<00:00, 318.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6260 [time] 0:00:00.182463\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 26/26 [00:00<00:00, 303.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6300 [time] 0:00:00.275029\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 26/26 [00:00<00:00, 300.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6387 [time] 0:00:00.370079\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 26/26 [00:00<00:00, 283.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6260 [time] 0:00:00.469796\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 26/26 [00:00<00:00, 310.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6347 [time] 0:00:00.561598\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 26/26 [00:00<00:00, 306.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6333 [time] 0:00:00.653941\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 26/26 [00:00<00:00, 303.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6340 [time] 0:00:00.746652\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 26/26 [00:00<00:00, 312.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6293 [time] 0:00:00.837251\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 26/26 [00:00<00:00, 277.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6313 [time] 0:00:00.938682\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 26/26 [00:00<00:00, 309.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6400 [time] 0:00:01.029833\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 26/26 [00:00<00:00, 305.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6247 [time] 0:00:01.123154\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 26/26 [00:00<00:00, 293.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6253 [time] 0:00:01.219594\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 26/26 [00:00<00:00, 309.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6273 [time] 0:00:01.311247\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 26/26 [00:00<00:00, 293.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6300 [time] 0:00:01.407736\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 26/26 [00:00<00:00, 294.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6180 [time] 0:00:01.504283\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 26/26 [00:00<00:00, 315.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6293 [time] 0:00:01.594580\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 26/26 [00:00<00:00, 308.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6300 [time] 0:00:01.687571\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 26/26 [00:00<00:00, 311.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6273 [time] 0:00:01.779710\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 26/26 [00:00<00:00, 271.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6227 [time] 0:00:01.884808\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████| 26/26 [00:00<00:00, 278.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6200 [time] 0:00:01.986599\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|██████████| 26/26 [00:00<00:00, 283.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6273 [time] 0:00:02.086197\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|██████████| 26/26 [00:00<00:00, 325.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6247 [time] 0:00:02.174031\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|██████████| 26/26 [00:00<00:00, 304.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6380 [time] 0:00:02.266648\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|██████████| 26/26 [00:00<00:00, 293.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6233 [time] 0:00:02.361806\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|██████████| 26/26 [00:00<00:00, 321.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6133 [time] 0:00:02.449261\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|██████████| 26/26 [00:00<00:00, 316.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6113 [time] 0:00:02.538756\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|██████████| 26/26 [00:00<00:00, 312.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6247 [time] 0:00:02.628570\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|██████████| 26/26 [00:00<00:00, 316.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6293 [time] 0:00:02.717503\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|██████████| 26/26 [00:00<00:00, 329.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.6233 [time] 0:00:02.803523\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|██████████| 26/26 [00:00<00:00, 318.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping triggered. Evaluating test score for the best validation score...\n",
            "\n",
            " the test score for epoch 30 is [0.657, 0.657, 0.657, 0.657, 0.657]\n",
            "Best validation score: 0.6467\n",
            "Averaged Test score over 5 runs: 0.6570\n",
            "\n",
            "\n",
            "Result:\n",
            "Best Epoch: 0\n",
            "Validation Score: 0.6467\n",
            "Averaged Test Score: 0.6570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Y8DTybxkLO_D",
        "outputId": "2ca336b2-e480-49f1-c7f1-354273fcfef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: CUDA\n",
            "----------------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 125/125 [00:01<00:00, 73.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8466 (test) 0.8458 [time] 0:00:02.021044\n",
            "🌸 New best epoch! 🌸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 125/125 [00:01<00:00, 75.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8555 (test) 0.8549 [time] 0:00:04.002714\n",
            "🌸 New best epoch! 🌸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 125/125 [00:01<00:00, 75.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8522 (test) 0.8498 [time] 0:00:05.988895\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 125/125 [00:01<00:00, 75.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8582 (test) 0.8565 [time] 0:00:07.974658\n",
            "🌸 New best epoch! 🌸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 125/125 [00:01<00:00, 75.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8624 (test) 0.8622 [time] 0:00:09.974212\n",
            "🌸 New best epoch! 🌸\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 125/125 [00:01<00:00, 74.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8575 (test) 0.8592 [time] 0:00:11.993332\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 125/125 [00:01<00:00, 74.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8592 (test) 0.8596 [time] 0:00:14.012918\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 125/125 [00:01<00:00, 74.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8585 (test) 0.8567 [time] 0:00:16.030653\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 125/125 [00:01<00:00, 74.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8615 (test) 0.8599 [time] 0:00:18.048731\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 125/125 [00:01<00:00, 74.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8557 (test) 0.8545 [time] 0:00:20.062073\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 125/125 [00:01<00:00, 75.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8600 (test) 0.8585 [time] 0:00:22.060763\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 125/125 [00:01<00:00, 75.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8578 (test) 0.8594 [time] 0:00:24.055356\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 125/125 [00:01<00:00, 76.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8500 (test) 0.8501 [time] 0:00:26.031260\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 125/125 [00:01<00:00, 76.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8395 (test) 0.8387 [time] 0:00:27.998452\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 125/125 [00:01<00:00, 76.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8527 (test) 0.8538 [time] 0:00:29.960950\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 125/125 [00:01<00:00, 76.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8519 (test) 0.8561 [time] 0:00:31.925696\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 125/125 [00:01<00:00, 76.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8545 (test) 0.8556 [time] 0:00:33.881496\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 125/125 [00:01<00:00, 76.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8536 (test) 0.8555 [time] 0:00:35.830254\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 125/125 [00:01<00:00, 77.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8575 (test) 0.8595 [time] 0:00:37.773218\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 125/125 [00:01<00:00, 76.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8596 (test) 0.8570 [time] 0:00:39.711747\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████| 125/125 [00:01<00:00, 77.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(val) 0.8600 (test) 0.8576 [time] 0:00:41.638404\n",
            "\n",
            "\n",
            "Result:\n",
            "{'val': 0.8624266411901187, 'test': 0.8622172177295526, 'epoch': 4}\n"
          ]
        }
      ],
      "source": [
        "# For demonstration purposes (fast training and bad performance),\n",
        "# one can set smaller values:\n",
        "# n_epochs = 20\n",
        "# patience = 2\n",
        "n_epochs = 1_000_000_000\n",
        "patience = 16\n",
        "\n",
        "batch_size = 256\n",
        "epoch_size = math.ceil(len(train_idx) / batch_size)\n",
        "timer = delu.tools.Timer()\n",
        "early_stopping = delu.tools.EarlyStopping(patience, mode=\"max\")\n",
        "best = {\n",
        "    \"val\": -math.inf,\n",
        "    \"test\": None,\n",
        "    \"epoch\": -1,\n",
        "}\n",
        "\n",
        "print(f\"Device: {device.type.upper()}\")\n",
        "print(\"-\" * 88 + \"\\n\")\n",
        "timer.run()\n",
        "for epoch in range(n_epochs):\n",
        "    for batch in tqdm(\n",
        "        delu.iter_batches(data[\"train\"], batch_size, shuffle=True),\n",
        "        desc=f\"Epoch {epoch}\",\n",
        "        total=epoch_size,\n",
        "    ):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(apply_model(batch), batch[\"y\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    val_score = evaluate(\"val\")\n",
        "    #test_score = evaluate(\"test\")\n",
        "    #print(f\"(val) {val_score:.4f} (test) {test_score:.4f} [time] {timer}\")\n",
        "\n",
        "    early_stopping.update(val_score)\n",
        "    if early_stopping.should_stop():\n",
        "        break\n",
        "\n",
        "    if val_score > best[\"val\"]:\n",
        "        print(\"🌸 New best epoch! 🌸\")\n",
        "        #best = {\"val\": val_score, \"test\": test_score, \"epoch\": epoch}\n",
        "        best = {\"val\": val_score}\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"\\n\\nResult:\")\n",
        "print(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh2zvJ-yLO_E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}